{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMH/+2kwzW3OLR1Q47/vZWd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mazihell/T4ConstInter/blob/master/ConstInterpetradoires.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from bs4 import BeautifulSoup\n",
        "from urllib.request import Request, urlopen"
      ],
      "metadata": {
        "id": "vnSAiB1jqDj8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TkXwl0p1G6Gl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fa84dde4-45c7-4489-8419-c8c23bd2b55b"
      },
      "source": [
        "import nltk\n",
        "#tokenizer\n",
        "nltk.download('punkt')\n",
        "#tagger pré treinado em inglês\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "#palavras para exclusão\n",
        "nltk.download('stopwords')\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from string import punctuation\n",
        "from collections import defaultdict\n",
        "from nltk.probability import FreqDist\n",
        "from heapq import nlargest"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "_R3JNYRup0Wv"
      },
      "outputs": [],
      "source": [
        "#Artigos em ingles para donwload\n",
        "link1 = Request('https://link.springer.com/article/10.1007/s42979-021-00592-x',headers={'User-Agent': 'Mozilla/5.0'})\n",
        "link2 = Request('https://link.springer.com/article/10.1007/s42979-022-01371-y',headers={'User-Agent': 'Mozilla/5.0'})\n",
        "link3 = Request('https://www.ibm.com/cloud/learn/neural-networks',headers={'User-Agent': 'Mozilla/5.0'})\n",
        "link4 = Request('https://realpython.com/python-ai-neural-network/',headers={'User-Agent': 'Mozilla/5.0'})\n",
        "link5 = Request('https://monkeylearn.com/text-classification/',headers={'User-Agent': 'Mozilla/5.0'})"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#armazenando da pagina na variavel\n",
        "pagina1 = urlopen(link1).read().decode('utf-8', 'ignore')\n",
        "pagina2 = urlopen(link2).read().decode('utf-8', 'ignore')\n",
        "pagina3 = urlopen(link3).read().decode('utf-8', 'ignore')\n",
        "pagina4 = urlopen(link4).read().decode('utf-8', 'ignore')\n",
        "pagina5 = urlopen(link5).read().decode('utf-8', 'ignore')\n"
      ],
      "metadata": {
        "id": "Ky4-VeVgp8RT"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "soup1 = BeautifulSoup(pagina1, \"lxml\")\n",
        "soup2 = BeautifulSoup(pagina2, \"lxml\")\n",
        "soup3 = BeautifulSoup(pagina3, \"lxml\")\n",
        "soup4 = BeautifulSoup(pagina4, \"lxml\")\n",
        "soup5 = BeautifulSoup(pagina5, \"lxml\")\n",
        "#busca pelo ID do que precisa do html\n",
        "texto1 = soup1.find(id=\"Sec5-content\").text\n",
        "texto2 = soup2.find(id=\"Sec2-content\").text\n",
        "texto3 = soup3.find(id=\"layout-section-second\").text\n",
        "texto4 = soup4.find(id=\"artificial-intelligence-overview\").text\n",
        "texto5 = soup5.find(id=\"___gatsby\").text  \n",
        "#soup3.find_all(id=True)"
      ],
      "metadata": {
        "id": "zOWA6_8z3aQy"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#divisão do texto em sentenças\n",
        "sentencas1 = sent_tokenize(texto1)\n",
        "sentencas2 = sent_tokenize(texto2)\n",
        "sentencas3 = sent_tokenize(texto3)\n",
        "sentencas4 = sent_tokenize(texto4)\n",
        "sentencas5 = sent_tokenize(texto5)\n",
        "#divisão do texto em palavras\n",
        "palavras1 = word_tokenize(texto1.lower())\n",
        "palavras2 = word_tokenize(texto2.lower())\n",
        "palavras3 = word_tokenize(texto3.lower())\n",
        "palavras4 = word_tokenize(texto4.lower())\n",
        "palavras5 = word_tokenize(texto5.lower())"
      ],
      "metadata": {
        "id": "OwU5g__53eP4"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#stopwords do ingles para buscar o significado sintatico dentro da sentença (sem informações relevantes do sentido da palavra)\n",
        "#punctuation retira a pontuação\n",
        "#utilizando \"set\" (não permite elementos repetidos) e também compreensão da lista\n",
        "stopwords1 = set(stopwords.words('english') + list(punctuation))\n",
        "stopwords2 = set(stopwords.words('english') + list(punctuation))\n",
        "stopwords3 = set(stopwords.words('english') + list(punctuation))\n",
        "stopwords4 = set(stopwords.words('english') + list(punctuation))\n",
        "stopwords5 = set(stopwords.words('english') + list(punctuation))\n",
        "# #retirando o stopwords da lista de palavras\n",
        "palavras_sem_stopwords1 = [palavra for palavra in palavras1 if palavra not in stopwords1] \n",
        "palavras_sem_stopwords2 = [palavra for palavra in palavras2 if palavra not in stopwords2]\n",
        "palavras_sem_stopwords3 = [palavra for palavra in palavras3 if palavra not in stopwords3]\n",
        "palavras_sem_stopwords4 = [palavra for palavra in palavras4 if palavra not in stopwords4]\n",
        "palavras_sem_stopwords5 = [palavra for palavra in palavras5 if palavra not in stopwords5]"
      ],
      "metadata": {
        "id": "KH4V8xUn4Fw6"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#FreqDist cria uma distribuição de frequencia para essa lista de palavras e descobrir quais são as mais importantes.\n",
        "frequencia1 = FreqDist(palavras_sem_stopwords1)\n",
        "frequencia2 = FreqDist(palavras_sem_stopwords2)\n",
        "frequencia3 = FreqDist(palavras_sem_stopwords3)\n",
        "frequencia4 = FreqDist(palavras_sem_stopwords4)\n",
        "frequencia5 = FreqDist(palavras_sem_stopwords5)\n",
        "# #separa quais são as sentenças mais importantes do texto criando um \"score\" para cada sentença baseado no numero de vezes que uma palavra importantes se repete dentro dela\n",
        "# #para isso é utilizando a chamada defaultdict\n",
        "sentencas_importantes1 = defaultdict(int)\n",
        "sentencas_importantes2 = defaultdict(int)\n",
        "sentencas_importantes3 = defaultdict(int)\n",
        "sentencas_importantes4 = defaultdict(int)\n",
        "sentencas_importantes5 = defaultdict(int)"
      ],
      "metadata": {
        "id": "GSan7Wds4Nzp"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#percorre todas as sentenças e coleta todas as estatisticas populando (preenchendo) o dicionario com o indice da setença e a soma \n",
        "#da frequencia de cada palavra presente na sentença\n",
        "for i, sentenca1 in enumerate(sentencas1):\n",
        "  for palavra in word_tokenize(sentenca1.lower()):\n",
        "      if palavra in frequencia1:\n",
        "          sentencas_importantes1[i] += frequencia1[palavra]\n",
        "\n",
        "for i, sentenca2 in enumerate(sentencas2):\n",
        "  for palavra in word_tokenize(sentenca2.lower()):\n",
        "      if palavra in frequencia2:\n",
        "          sentencas_importantes2[i] += frequencia2[palavra]\n",
        "\n",
        "for i, sentenca3 in enumerate(sentencas3):\n",
        "  for palavra in word_tokenize(sentenca3.lower()):\n",
        "      if palavra in frequencia3:\n",
        "          sentencas_importantes3[i] += frequencia3[palavra]\n",
        "\n",
        "for i, sentenca4 in enumerate(sentencas4):\n",
        "  for palavra in word_tokenize(sentenca4.lower()):\n",
        "      if palavra in frequencia4:\n",
        "          sentencas_importantes4[i] += frequencia4[palavra]\n",
        "\n",
        "for i, sentenca5 in enumerate(sentencas5):\n",
        "  for palavra in word_tokenize(sentenca5.lower()):\n",
        "      if palavra in frequencia5:\n",
        "          sentencas_importantes5[i] += frequencia5[palavra]"
      ],
      "metadata": {
        "id": "E76dXiuz4UdA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#seleciona no dicionario as 'n' sentenças mais importantes para formar o resumo com isso é uitilizado o nlargest da biblioteca heapq\n",
        "#abaixo foi escolhido 5 setenças mais importantes\n",
        "idx_sentencas_importantes1 = nlargest(5, sentencas_importantes1, sentencas_importantes1.get)\n",
        "idx_sentencas_importantes2 = nlargest(5, sentencas_importantes2, sentencas_importantes2.get)\n",
        "idx_sentencas_importantes3 = nlargest(5, sentencas_importantes3, sentencas_importantes3.get)\n",
        "idx_sentencas_importantes4 = nlargest(5, sentencas_importantes4, sentencas_importantes4.get)\n",
        "idx_sentencas_importantes5 = nlargest(5, sentencas_importantes5, sentencas_importantes5.get)"
      ],
      "metadata": {
        "id": "X5_O9oFV4jsZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#visualiza as sentenças da pagina em html obs. as mais importantes conforme a frequencia das palavras\n",
        "for i in sorted(idx_sentencas_importantes1):\n",
        "    print(sentencas1[i])\n",
        "\n",
        "print(\"--------------------------------------------------\")\n",
        "\n",
        "for i in sorted(idx_sentencas_importantes2):\n",
        "    print(sentencas2[i])\n",
        "\n",
        "print(\"--------------------------------------------------\")\n",
        "\n",
        "for i in sorted(idx_sentencas_importantes3):\n",
        "    print(sentencas3[i])\n",
        "\n",
        "print(\"--------------------------------------------------\")\n",
        "\n",
        "for i in sorted(idx_sentencas_importantes4):\n",
        "    print(sentencas4[i])\n",
        "\n",
        "print(\"--------------------------------------------------\")\n",
        "\n",
        "for i in sorted(idx_sentencas_importantes5):\n",
        "    print(sentencas5[i])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OgdBHNy-4mRa",
        "outputId": "625a035e-04fe-4866-85c1-a25e5b7ac96a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "In this section, we discuss various machine learning algorithms that include classification analysis, regression analysis, data clustering, association rule learning, feature engineering for dimensionality reduction, as well as deep learning methods.\n",
            "3A general structure of a machine learning based predictive model considering both the training and testing phaseFull size imageClassification AnalysisClassification is regarded as a supervised learning method in machine learning, referring to a problem of predictive modeling as well, where a class label is predicted for a given example [41].\n",
            "Feature selection: The selection of features, also known as the selection of variables or attributes in the data, is the process of choosing a subset of unique features (variables, predictors) to use in building machine learning and data science model.\n",
            "RL can be used to solve numerous real-world problems in various fields, such as game theory, control theory, operations analysis, information theory, simulation-based optimization, manufacturing, supply chain logistics, multi-agent systems, swarm intelligence, aircraft control, robot motion control, and many more.Artificial Neural Network and Deep LearningDeep learning is part of a wider family of artificial neural networks (ANN)-based machine learning approaches with representation learning.\n",
            "[96].Overall, based on the learning techniques discussed above, we can conclude that various types of machine learning techniques, such as classification analysis, regression, data clustering, feature selection and extraction, and dimensionality reduction, association rule learning, reinforcement learning, or deep learning techniques, can play a significant role for various purposes according to their capabilities.\n",
            "--------------------------------------------------\n",
            "Recently, some studies have used interpretable ML in AD detection.For example, Long Short-Term Memory- (LSTM-) [11] based Recurrent Neural Networks (RNN) [12] were trained to classify CN vs. MCI subjects in [13].\n",
            "The resulting models were evaluated for two AD datasets—the AD subset [14] of the Heinz Nixdorf Risk Factors Evaluation of Coronary Calcification and Lifestyle (RECALL) (HNR) [15] (61 MCI and 59 CN) and 624 subjects (397 MCI, 227 CN) of the Alzheimer’s Disease Neuroimaging Initiative (ADNI) [16] study phase 1.\n",
            "The model outperformed SVMs and artificial neural networks.A new interpretable model based on distinct weighted rules was introduced in [26] and evaluated for 151 subjects (97 AD and 54 CN) of the ADNI cohort.\n",
            "SHAP summary plots showed that models trained for both the entire training set and the coreset learned biologically plausible associations.To examine the predictive influence of \\(\\upbeta\\)-amyloid plaques, tau tangles, and neurodegeneration during the disease progression, RF feature importance was used in [31].\n",
            "Transfer learning applied information extracted from the Survey of Health, Ageing, and Retirement in Europe (SHARE) [34] (80,699 CN, 4,157 AD) to the PREVENT cohort [35] (109 subjects with high risk to develop AD, 364 subjects with low risk).\n",
            "--------------------------------------------------\n",
            "\n",
            "  Neural Networks\n",
            "\n",
            "\n",
            "\n",
            "Neural networks reflect the behavior of the human brain, allowing computer programs to recognize patterns and solve common problems in the fields of AI, machine learning, and deep learning.\n",
            "Neural networks, also known as artificial neural networks (ANNs) or simulated neural networks (SNNs), are a subset of machine learning and are at the heart of deep learning algorithms.\n",
            "Since neural networks behave similarly to decision trees, cascading data from one node to another, having x values between 0 and 1 will reduce the impact of any given change of a single variable on the output of any given node, and subsequently, the output of the neural network.\n",
            "Deep Learning vs. Neural Networks: What’s the Difference?”\n",
            "History of neural networks\n",
            "The history of neural networks is longer than most people think.\n",
            "While the idea of “a machine that thinks” can be traced to the Ancient Greeks, we’ll focus on the key events that led to the evolution of thinking around neural networks, which has ebbed and flowed in popularity over the years:\n",
            "1943: Warren S. McCulloch and Walter Pitts published “A logical calculus of the ideas immanent in nervous activity (PDF, 1 MB) (link resides outside IBM)” This research sought to understand how the human brain could produce complex patterns through connected brain cells, or neurons.\n",
            "--------------------------------------------------\n",
            "Getting back to the sudoku example in the previous section, to solve the problem using machine learning, you would gather data from solved sudoku games and train a statistical model.\n",
            "The image below presents the workflow to train a model using supervised learning:\n",
            "Workflow to train a machine learning model\n",
            "The combination of the training data with the machine learning algorithm creates the model.\n",
            "For example, inflected forms of the verb “watch,” like “watches,” “watching,” and “watched,” would be reduced to their lemma, or base form: “watch.” \n",
            "If you’re using arrays to store each word of a corpus, then by applying lemmatization, you end up with a less-sparse matrix.\n",
            "The following image presents the process of lemmatization and representation using a bag-of-words model:\n",
            "Creating features using a bag-of-words model\n",
            "First, the inflected form of every word is reduced to its lemma.\n",
            "Deep Learning\n",
            "Deep learning is a technique in which you let the neural network figure out by itself which features are important instead of applying feature engineering techniques.\n",
            "--------------------------------------------------\n",
            "In this article, I tell you how to create a neural network model using TensorFlow.I think this will help you get some idea.\n",
            "Here at the start I just follow the string conversion and directly applied data into the model.\n",
            "But you need to Follow the pre-processing steps in the Model Creation.\n",
            "Here I just try to give a simple idea about model creation.\n",
            "I can tell that you need to spend more time on the data preprocessing steps.\n"
          ]
        }
      ]
    }
  ]
}